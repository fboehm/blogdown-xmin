---
title: R code profiling saves the day!
author: Frederick Boehm
date: '2017-10-16'
slug: r-code-profiling-saves-the-day
draft: 'false'
categories:
  - computing
tags:
  - R
  - high-throughput computing
---

## Overview

I recently needed to write R code to fit a linear model using generalized least squares (GLS). My initial attempt at writing functions to do this, while technically correct, seemed to be slow. For example, fitting about 100 such models took nearly 30 minutes on my MacBook Pro computer.

My graduate school advisors suggested that I re-examine my code to see if I could find inefficiencies. For instance, if one were fitting multiple models with ordinary least squares (OLS), and the models all have the same design matrix, but distinct response vectors, then one could save computations by calculating only once the matrix $$(X^TX)^{-1}X^T$$ and merely plugging in the response vector many times, using matrix multiplication, to get the OLS estimators for each model:

$$\hat \beta = (X^TX)^{-1}X^TY$$

I needed to use GLS - rather than OLS - because my response vector has a complicated (known) covariance structure. Specifically, my linear model is 

$$vec(Y) = Xvec(B) + vec(G) + vec(E)$$ where $Y$ is a n by 2 matrix of two phenotype measurements on each of $n$ mice, $X$ is a 2n by 2f block-diagonal design matrix, with two blocks each of size n by f, $G$ is a n by 2 matrix of genetic random effects, and $E$ is a n by 2 matrix of random errors. $vec()$ denotes the vectorization operation, which, for us, means that we stack the columns, *i.e.*, $vec(Y)$ is a length-2n vector that has the first n entries equal to the first column of $Y$ and the second n entries equal to the second column of $Y$. 

Note that the $X$ matrix contains genotype probabilities for two distinct genetic markers. In our case, these markers are single nucleotide polymorphisms (SNPs). Each mouse in our study is a genetic mosaic of 8 inbred founder lines, so we have 8 possible founder alleles for each marker. For each mouse and each locus, we've inferred the eight founder genotype probabilities via hidden Markov models. 

I need to specify the assumed distributions for $G$ and $E$. 

We assume $$E \sim MN_{nx2}(0, I_n, V_e)$$ and $$G \sim MN_{nx2}(0, K, V_g)$$ where $MN_{nx2}(A, R, C)$ denotes the matrix-variate normal distribution for a n by 2 matrix with mean matrix $A$, among-rows, n by n covariance matrix $R$, and among-columns, 2 by 2 covariance matrix $C$. Note that $0$, here, refers to the n by 2 matrix the entries of which are all zeros. Note also that I use the phrase among-rows covariance to the covariance that relates the row vectors, while among-columns covariance refers to the covariance matrix that relates the column vectors. $I_n$ is the n by n identity matrix, while 

We use known properties of the Kronecker product to write the equivalent distributions for $vec(G)$ and $vec(E)$. 

We get that 

$$vec(G) \sim N(0, V_g \otimes K)$$ and $$vec(E) \sim N(0, V_e \otimes I_n)$$ where $\otimes$ denotes the Kronecker product. Note that the Kronecker product is not commutative, *i.e.*, so $$A \otimes B \neq B \otimes A$$

However, the two Kronecker products $A \otimes B$ and $B \otimes A$ have the same dimensions, so one must be extremely careful to get the ordering correct when writing them. 

## A small distraction: Correcting the ordering for Kronecker products

In writing this blog post, I realized that I've been writing my Kronecker products backwards, *i.e.*, I've been writing in R `In %x% Ve` when I should be writing `Ve %x% In`. I know that I've always written my Kronecker products with `%x%` rather than using an explicit call to the function `kronecker()`, but, without looking, I'm not sure which functions in my package [`qtl2pleio`](https://github.com/fboehm/qtl2pleio). For this reason, I'll use the linux command line tool `grep`. Specifically, I write:

```{bash, cache = TRUE}
grep '%x%' ~/Box\ Sync/Rpkgs/qtl2pleio/R/*.R
```

I see that there are two lines that feature the `%x%` operator. I see also that my function `sim1` needs to be changed so that it calls a new-ish function `calc_Sigma`. 

I make those changes in the two files, and then use `grep` again to ensure that the function `calc_Sigma` has the proper ordering of components in the Kronecker product. 

```{bash, cache = FALSE}
grep '%x%' ~/Box\ Sync/Rpkgs/qtl2pleio/R/*.R
```

A reference that discusses the ordering of components in Kronecker products when using the $vec()$ operator is [this Wikipedia article](https://en.wikipedia.org/wiki/Matrix_normal_distribution).

*Aside*: Note that I had to set the R code chunk option `cache=TRUE` for the first `bash` chunk in my Rmd source file to enable preservation of the 'bad' code after its correction. 

## Back to the main point: Profiling!

Ok, now that I've fixed that error, I return to the heart of the story. I need to fit many (tens of thousands) of linear models, each with different design matrices. However, I have only a limited number of phenotypes. For a single bivariate phenotype, *i.e.*, a single n by 2 $Y$ matrix of phenotype measurements, I need to fit approximately 10,000 distinct linear models, where the linear models differ in their design matrices. The key realization is that the covariance structure is the same for all models that use a single bivariate phenotype. Thus, while I need to calculate $$vec(\hat B) = (X^T\Sigma^{-1} X)^{-1}X^T\Sigma^{-1}vec(Y)$$ for each distinct design matrix, **I don't need to calculate $\Sigma^{-1}$ for every linear model, since I am treating $\Sigma$ as fixed for a given bivariate phenotype.**

That is, I really only need to calculate $\Sigma^{-1}$ once for each bivariate phenotype because I assume that $\Sigma$ depends only on the bivariate phenotype. *I should add that this is an approximation to the correct model fit, but, for my purposes, I believe that it's good enough.* 

I initially wrote a function that calculated the inverse of $\Sigma$ twice on the fly. Profiling helped me to identify that it was the process of using `solve(Sigma)` that consumed so much of my computing time, *i.e.*, when I was using about 30 minutes to fit 100 models. 

I naively wrote:

```{r, eval = FALSE}
Bhat <- solve(t(X) %*% solve(Sigma) %*% X) %*% t(X) %*% solve(Sigma) %*% as.vector(Y)
```


Specifically, my graduate school advisors pointed out that the book [Advanced R](http://adv-r.had.co.nz) has a [chapter on profiling](http://adv-r.had.co.nz/Profiling.html).

I then used Hadley Wickham's `lineprof` R package to identify the lines in my functions that took the greatest amount of time. All of the most time-consuming lines contained a call to `solve`. In examining my code closely, I realized that I could rewrite it so that I called `solve` only once for per bivariate phenotype, rather than twice per model fit. 

I then changed my code so that, for each bivariate phenotype I calculate `Sigma_inv` only once:

```{r, eval = FALSE}
solve(Sigma) -> Sigma_inv
for (i in 1:n_snp){
  for (j in 1:n_snp){
    Bhat <- solve(t(X) %*% Sigma_inv %*% X) %*% t(X) %*% Sigma_inv %*% as.vector(Y)
  }
}
```

*Note that I've omitted some of the code in the above chunk. Despite this, I think that the point is well illustrated.*

I have sped up my code more than 10x by removing the repeated evaluations of `solve(Sigma)`.

Wickham discusses bottleneck identification via line profiling in his chapter on profiling. This Advanced R chapter is well written and makes for good reading. I highly recommend it. 





